---
title: "Take-Home Exercise 2: Applied Spatial Interaction Models - A Case Study of Singapore Public Bus Commuter Flows"
author: "Goh Si Hui"
date: 2023/12/08
date-format: long
date-modified: "last-modified"
format: html 
execute: 
  echo: true
  eval: true
  warning: false
editor: visual 
---

## Overview

### Introduction

What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These are some questions faced by transport operators and urban managers.

To answer these many questions related to urban mobility, traditionally commuters survey would be used. However, conducting commuters survey is costly, time-consuming and laborious. In addition, survey data tend to take a long time to clean and analyse, which can lead to the collected information being out-of-date when the survey report was ready.

With the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters, we will utilise and transform geospatially-referenced data into insightful information, hence generating returns on the investment made to collect and manage this data.

### Objectives

In this take-home exercise, we will be using publicly available data and geospatial data science and analysis to answer questions related to urban mobility and how it can be used to support decision-making. In particular, we will be building spatial interaction models to determine factors affecting urban mobility patterns of public bus transit.

### Packages Used

For this exercise, we will be using the following packages:

-   **sf** : to import, integrate, process and transform geospatial data

-   **sp**: to provide classes and methods for spatial data types

-   **DT**:to display R data objects (matrices or dataframes) as tables on HTML pages

-   **stplanr**: to visualise flow lines

-   **performance**:to compute measures for assessing model quality

-   **reshape2**:to restructure and aggregate data

-   **tidyverse**: to import, integrate, wrangle and visualise data

-   **tmap**: to create thematic maps

-   **ggpubr**: to create and customise 'ggplot2'- based publication ready plots

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

pacman::p_load(tmap, sf, sp, DT, stplanr, performance, reshape2, ggpubr, tidyverse) 
```

## Data Preparation

For the purpose of this take-home exercise, we will be using the following datasets downloaded from [LTA Datamall](https://datamall.lta.gov.sg/content/datamall/en.html), [Data.gov.sg](https://beta.data.gov.sg/) and datasets provided by the course instructor.

-   Geospatial Data

    -   `Bus Stop Location`

    -   `Train Station Exit Point`

    -   `Master Plan 2019 Subzone Boundary`

    -   `Business`, 
    
    -   `FinServ`, 

-   Aspatial Data

    -   `Passenger Volume by Origin Destination Bus Stops`
    
    -   `hdb`: contains the location of existing HDB blocks, highest floor, year of completion, type of building and number of HDB flats, total dwelling units, numer of flats sold by flat type  
    
    -   `schools`

### Importing Data into R

::: {.panel-tabset}

## Passenger Volume from Origin Busstops (odbus)

```{r}

odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```
## HDB Property Information 
```{r}

hdb <- read_csv("data/aspatial/hdb.csv")

```
## Schools Information 
```{r}

schools <- read_csv("data/aspatial/schools1.csv")

```
## Busstops Information 
```{r}
busstop <- st_read(dsn = "data/geospatial",
                   layer = "BusStop")

```

## Master Plan 2019 Planning Subzone 
```{r}
mpsz <- st_read(dsn = "data/geospatial",
                   layer = "MPSZ-2019")
```

## Location of Businesses 

```{r}
business <- st_read(dsn = "data/geospatial",
                   layer = "Business")

```
## Location of Financial Services 
```{r}
finserv <- st_read(dsn = "data/geospatial",
                   layer = "FinServ")

```
## Location of MRT Exits 
```{r}

mrtexits <- st_read(dsn = "data/geospatial",
                   layer = "Train_Station_Exit_Layer")

```
::: 



### Data Wrangling of Aspatial Data

#### Passenger Volumes from Origin Busstops 

```{r}
#| code-fold: true
#| code-summary: "Show the code"
glimpse(odbus)
```
From the above output, it shows that the values of the ORIGIN_PT_CODE and DESTINATION_PT_CODE are in character data type. Since these two columns contain the origin bus stop codes and destination bus stop codes and they actually take on a limited number of different values (unlike strings), we convert ORIGIN_PT_CODE and DESTINATION_PT_CODE into factor data type using as.factor().

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE)

```

Let us check the odbus dataframe using glimpse() again. 

```{r}
#| code-fold: true
#| code-summary: "Show the code"
glimpse(odbus)

```
ORIGIN_PT_CODE and DESTINATION_PT_CODE have been converted to factor data type. 

Next, we will do a check on the data. Using unique() function, we can find the unique values in a column. Let us first check the YEAR_MONTH, DAY_TYPE, TIME_PER_HOUR and PT_TYPE to ensure the data contains bus trips made in October 2023, and there are no other unexpected values for DAY_TYPE and TIME_PER_HOUR.

:::{.panel-tabset}
## YEAR_MONTH

```{r}
unique(odbus$YEAR_MONTH)
```

## DAY_TYPE

```{r}
unique(odbus$DAY_TYPE)
```

## TIME_PER_HOUR

```{r}
unique(odbus$TIME_PER_HOUR)

#To count the number hours listed
length(unique(odbus$TIME_PER_HOUR))

```

## PT_TYPE

```{r}
unique(odbus$PT_TYPE)
```
::: 

From the above output, we know that the data only contains **bus** trips made in **October 2023**. The DAY_TYPES have 2 categories: **WEEKDAYS** and **WEEKENDS/HOLIDAY**. TIME_PER_HOUR have **23** categories because there is no value of "3" in TIME_PER_HOUR variable.

#### Extract the time period we are interested in - Weekday Morning Peak Hour

For the purpose of this exercise, we are interested in the bus commuter flows on weekdays morning peak hour (i.e. 6am to 9am) so we will use the following code chunk to extract the time period we are interested in and save the output in rds format for future use. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
odbus_WDMP <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE, 
           DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

Let us check the extracted passenger volume from origin busstops on weekday morning peak hour. 


```{r}
summary(odbus_WDMP)

```
We will use the following code chunk to save this dataframe in rds format for future use. 

```{r}
write_rds(odbus_WDMP, "data/rds/odbus_WDMP.rds")
```

#### HDB Data 

We will now take a look at the HDB data. 

```{r}
#| code-fold: true
#| code-summary: "Show the code"
glimpse(hdb)
```

From the above output, we know that there are some HDB buildings considered as residential, commercial, market, pavillion and even carparks. We will filter the residential and commercial HDB buildings out and save them as different dataframes because these could be propulsive or attractive factors for people to travel in morning peak hour (e.g. people would want to move away from residential areas and move towards commercial buildings in the morning for work).  

::: {.panel-tabset}

## Code 
```{r}
hdb_residential <- hdb %>%
  filter(residential == "Y") %>%
  select(postal, lat, lng, SUBZONE_N, residential, total_dwelling_units)
```

## Output
```{r}
glimpse(hdb_residential)
```
:::

We check if there are any duplicates in the hdb_residential dataset using the following code chunk. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
duplicate <- hdb_residential %>%
  group_by_all() %>% 
  filter(n()>1) %>%
  ungroup()

duplicate
```

From the above output, we know that there is no duplicate rows in the hdb_residential dataframe.

We will now extract the data for HDB Commercial Buildings. 

::: {.panel-tabset}

## Code 

```{r}
hdb_commercial <- hdb %>%
  filter(commercial == "Y") %>%
  select(postal, lat, lng, SUBZONE_N, commercial, total_dwelling_units)
```

## Output

```{r}
glimpse(hdb_commercial)
```
:::

We check if there are any duplicates in the hdb_commercial dataset using the following code chunk. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

duplicate1 <- hdb_commercial %>%
  group_by_all() %>% 
  filter(n()>1) %>%
  ungroup()

duplicate1
```
From the output we see that there are no duplicates. 

Let us check further to see if there are any buildings with same postal code but occupy 2 different longitude and latitude. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

same_postal <- hdb_commercial %>%
  group_by(postal) %>% 
  filter(n()>1) %>%
  ungroup()

same_postal
```
From the output, we find that there are postal code 080001 occupies 2 different geographical locations and one location has no commercial dwellings while another location has 95 dwelling units. For this analysis, since we are going to analyse the commuter flows at the analytical hexagon level, I choose to retain these 2 geographical location despite them sharing the same postal code. This is because (1) they could end up in 2 different hexagon grids and (2) removing 1 and not the other could cause us to misinterpret the information. 
For the remaining rows, they are different geographical locations but do not have a postal code.  I choose to retain these geographical locations despite them not having postal code because we will be using their latitude and longitude in the subsequent steps to convert it into a simple feature data frame and they will give us the point geometry point coordinates. 



#### Schools Information 

We will now take a look at the imported schools data.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

glimpse(schools)

```

There are a lot of information that we do not need from the imported data (e.g. School principal) so we will use the following code chunk to retain the columns that we want and also rename the results.LATITUDE and results.LONGITUDE columns to latitude and longitude respectively.  

:::{.panel-tabset}

## Codes to select the columns we want 

```{r}

schools_tidy <- schools %>%
  rename(latitude = "results.LATITUDE",
         longitude = "results.LONGITUDE")%>%
  select(postal_code, school_name, latitude, longitude)
```

## Output 

```{r}

glimpse(schools_tidy)

```

:::

We will now check if there are any duplicates in the schools_tidy data frame. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

duplicate2 <- schools_tidy %>%
  group_by_all() %>% 
  filter(n()>1) %>%
  ungroup()

duplicate2
```

From the above output, we find that there are duplicate entries for Singapore Chinese Girls' Primary School, Singapore Chinese Girls' School, Methodist Girls' School (Secondary) and Methodist Girls' School (Primary). Hence we will remove the duplicates using the following code chunk. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

schools_tidy <- unique(schools_tidy)
```

Let us do a check to confirm that there are no duplicates in the data frame now. 
```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

duplicate3 <- schools_tidy %>%
  group_by_all() %>% 
  filter(n()>1) %>%
  ungroup()

duplicate3
```
From the above output, it shows that there are no more duplicates in the dataframe. 

#### Converting Aspatial data into SF Tibble Dataframe

Currently the hdb residential, hdb commercial and school_tidy data are still asaptial data even though they have latitude and longitude information. So we will use st_as_sf() to convert it into an sf tibble dataframe and use st_transform() to change it to SYV21 projected coordinate system (i.e. the crs code is 3414)


We use the following code chunk to convert hdb residential data into an sf tibble dataframe. 

:::{.panel-tabset}

## Codes 

```{r}
hdb_residental_sf <- st_as_sf(hdb_residential, 
                              coords = c("lng", "lat"),
                              crs=4326) %>%
  st_transform(crs = 3414)
```

## Output 

```{r}

glimpse(hdb_residental_sf)
```

::: 

We use the following code chunk to convert hdb commercial data into an sf tibble dataframe. 

:::{.panel-tabset}

## Codes 

```{r}
hdb_commercial_sf <- st_as_sf(hdb_commercial, 
                              coords = c("lng", "lat"),
                              crs=4326) %>%
  st_transform(crs = 3414)
```
## Output 
```{r}

glimpse(hdb_commercial_sf)
```
:::

We use the following code chunk to convert schools_tidy into an sf tibble dataframe. 

:::{.panel-tabset}

## Codes 
```{r}
schools_sf <- st_as_sf(schools_tidy,
                       coords = c("longitude", "latitude"), 
                       crs=4326) %>%
  st_transform(crs=3414)
  
```

## Output 
```{r}
glimpse(schools_sf)

```
:::


### Data Wrangling of Spatial Data

We will now take a look at the spatial data imported in. 

:::{.panel-tabset}

### mpsz 
```{r}
glimpse(mpsz)
```

### busstop 
```{r}
glimpse(busstop)
```

### TrainExits 
```{r}
glimpse(mrtexits)
```

### business
```{r}
glimpse(business)
```

### financial services
```{r}
glimpse(finserv)
```

::: 

We noticed that mpsz dataset is made up of multipolygons while the remaining datasets are made up of point geometry. 

Let us now check the coordinates system using the st_crs() function. 

:::{.panel-tabset}

### mpsz 
```{r}
st_crs(mpsz)
```

### busstop 
```{r}
st_crs(busstop)
```

### TrainExits 
```{r}
st_crs(mrtexits)
```

### business
```{r}
st_crs(business)
```

### Financial Services
```{r}
st_crs(finserv)
```

::: 

From the above output, we see that the coordinate system for mpsz is WGS 84, and the coorinate systems for the remaining datasets are SVY21. However, busstop and trainexits still have '9001' in the output of st_crs(), meaning that this is a wrong EPSG code because the correct EPSG code for SVY21 should be 3414.

We use st_transform transform the CRS for mpsz because it was in another projection format. Then we use st_set_crs() to assign the correct ESPG code to the various sf dataframe. 

:::{.panel-tabset}

### mpsz 
```{r}
mpsz3414 <- st_transform(mpsz, crs=3414)
st_crs(mpsz3414)
```

### Bus Stop 
```{r}
busstop3414 <- st_set_crs(busstop, 3414)
st_crs(busstop3414)
```

### Train Exits 
```{r}
mrtexits3414 <- st_set_crs(mrtexits, 3414)
st_crs(mrtexits3414)
```

### Business
```{r}
business3414 <- st_set_crs(business, 3414)
st_crs(business3414)
```

### Financial Services
```{r}
finserv3414 <- st_set_crs(finserv, 3414)
st_crs(finserv3414)
```

::: 

#### Creating the Analytical Hexagon Layer 

For this exercise, we will be analysing the bus commuter flows at analytics hexagon level. The analytical hexagon data is 375m between the centre of the hexagon and its edges to represent the traffic analysis zone. 

First, we will create a hexagon tessellation using MPSZ3414 to define the bounding box of the tessellation. As the hexagon layer that we want is 375m (the perpendicular distance between the centre of the hexagon and its edge), and the cellsize parameter in st_make_grid() requires the length to be between opposite edges, we multiply 375m by 2 to get 750m, which is the distance between 2 opposite edges of a hexagon. 

In the code chunk below, we also specify the crs = 3414 since we are using the SVY21 coordinate reference system and set square = FALSE to return a hexagon grid (rather than a square grid).

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hex_grid <- st_make_grid(mpsz3414, c(750,750), crs= 3414, what = "polygons", square = FALSE)

head(hex_grid, 10)

```
Next, we convert hex_grid from a sfc_POLYGON object to sf object for easier handling and add an unique ID to each cell. Each cell is a hexagon and in subsequent steps, we will check the number of trips, count the number of businesses, financial services, schools, residential units, hdb commercial units within each hexagon.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hex_grid_sf <- st_sf(hex_grid) %>%
  mutate(grid_id = 1:length(lengths(hex_grid)))

head(hex_grid_sf, 10)
```
This will give us a hexagon grid as seen below. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
qtm(hex_grid_sf)
```

To make the entire Singapore map into a hexagon grid, we use st_intersects() to find out the "overlap" between the entire hexagon grid and the Singapore map (as outlined by mpsz3414). We use lengths() to return the number of each element of the list then attach it as a column back to hex_grid_sf data frame. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hex_grid_sf$SG <- lengths(st_intersects(hex_grid_sf, mpsz3414))

```

We then use this column to "cut out" the hexagon grid in Singapore map shape by filtering out values in the SG column that do not contain 0 because 0 means there is no overlap between mpsz and the hexagon grid. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
sgmap <- filter(hex_grid_sf, SG>0)

head(sgmap, 10) 

```
We plot out `sgmap` to check that the entire Singapore map is now broken into hexagon units, rather than planning subzones. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
qtm(sgmap)
```

We use st_intersects() and lengths() to find out how many businesses, Financial Services, MRT Exits, Busstops, HDB Commercial buildings, HDB Residential Buildings are in each hexagon.  

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
sgmap$`BIZ_COUNT` <- lengths(st_intersects(sgmap, business3414))
sgmap$`FINSERV_COUNT` <- lengths(st_intersects(sgmap, finserv3414))
sgmap$`MRTEXIT_COUNT` <- lengths(st_intersects(sgmap, mrtexits3414))
sgmap$`BUSSTOPS_COUNT` <- lengths(st_intersects(sgmap, busstop3414))
sgmap$`HDBCOMMERCIAL_COUNT` <- lengths(st_intersects(sgmap, hdb_commercial_sf))
sgmap$`HDBRESIDENTIAL_COUNT` <- lengths(st_intersects(sgmap, hdb_residental_sf))
sgmap$`SCHOOLS_COUNT` <- lengths(st_intersects(sgmap, schools_sf))

glimpse(sgmap)
```

Notice that the sgmap has hexagon related information and the counts of the businesses, financial services, HDB commercial buildings, HDB residential buildings, schools, MRT exits and bus stops in each hexagon, but we are unable to map it back to the odbus_WDMP data because there are no common fields. 

Hence, we will use st_intersection() to perform point (busstop3414) and polygon (sgmap) overlay. The output will be in point sf object. 


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
busstop_hex <- st_intersection(busstop3414, sgmap)
glimpse(busstop_hex)
```
The output shows that we now know which bus stops are in which grid using the grid id. 

We also intersect mpsz with sgmap to get the subzones and their corresponding grid id. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
mpsz_hex <- st_intersection(mpsz3414, sgmap)
glimpse(mpsz_hex)

```
We then combine busstop_hex and mpsz_hex to get the busstops, their grid_id, the subzones they are in and the count of the various factors affecting commuter flows 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
busstop_hex_mpsz <- st_intersection(busstop_hex, mpsz_hex) %>%
  select(BUS_STOP_N,LOC_DESC,
         grid_id,BIZ_COUNT,
         FINSERV_COUNT,MRTEXIT_COUNT,BUSSTOPS_COUNT, 
         HDBCOMMERCIAL_COUNT,HDBRESIDENTIAL_COUNT,
         SCHOOLS_COUNT,SUBZONE_N,SUBZONE_C)

glimpse(busstop_hex_mpsz)
```

::: {.callout-note}

Note that after we combined busstop_hex with mpsz_hex, the number of observations dropped from 5161 to 5156. This is because there are 5 busstops outside of Singapore. 

:::


### Visualising the Attractiveness/ Propulsiveness Factors 

We use the following code chunk to visualise the distribution of businesses over the hexagon grid of singapore map. 

:::{.panel-tabset} 

## Summary of BIZ_COUNT 

```{r}
summary(sgmap$BIZ_COUNT)
```
## Distribution of Businesses

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

tmap_mode("plot")

  tm_shape(sgmap) +
  tm_fill("BIZ_COUNT", title = "Number of Businesses") + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Number of Businesses Per Grid",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE)  

```

## Data Table
```{r}
datatable(sgmap %>% 
  select(grid_id,BIZ_COUNT) %>%
  arrange(desc(sgmap$BIZ_COUNT)))

```
:::


From the above output, we observe that grid 2419 has the highest number of businesses concentrated, followed by grid 1673, 717 and 2009. Overall, the western part of Singapore has a larger concentration of businesses. This could be due to there having more factories and industrial estates in that area.  

:::{.panel-tabset} 

## Summary of FINSERV_COUNT 

```{r}
summary(sgmap$FINSERV_COUNT)
```

## Distribution of Financial Services
```{r}

tmap_mode("plot")
tm_shape(sgmap) +
  tm_fill("FINSERV_COUNT", title = "Number of Financial Services") + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Number of Financial Services Per Grid",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 


```
## Data Table
```{r}
datatable(sgmap %>% 
  select(grid_id,FINSERV_COUNT) %>%
  arrange(desc(sgmap$FINSERV_COUNT)))

```

:::


:::{.panel-tabset} 

## Summary of MRTEXIT_COUNT 

```{r}
summary(sgmap$MRTEXIT_COUNT)
```
## Distribution of MRT Exits
```{r}

tmap_mode("plot")
tm_shape(sgmap) +
  tm_fill("MRTEXIT_COUNT", title = "Number of MRT Exits") + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Number of MRT EXIT Per Grid",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 

```
## Data Table
```{r}
datatable(sgmap %>% 
  select(grid_id,MRTEXIT_COUNT) %>%
  arrange(desc(sgmap$MRTEXIT_COUNT)))

```
:::


:::{.panel-tabset} 
## Summary of BUSSTOPS_COUNT 

```{r}
summary(sgmap$BUSSTOPS_COUNT)
```

## Distribution of Bus Stops 
```{r}

tmap_mode("plot")
tm_shape(sgmap) +
  tm_fill("BUSSTOPS_COUNT", title = "Number of Bus Stops") + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Number of Bus Stops Per Grid",
            main.title.position = "center",
            main.title.size = 1,
            legend.height = 0.35, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) 

```

## Data Table
```{r}
datatable(sgmap %>% 
  select(grid_id,BUSSTOPS_COUNT) %>%
  arrange(desc(sgmap$BUSSTOPS_COUNT)))

```
:::




## Creating O-D Matrix 

To visualise the commuter flows from orgin busstops to destination busstops, we need to create an Origin-Destination Matrix. 
First, we will convert the busstop_hex_mpsz (sf dataframe) to a tibble dataframe using st_drop_geometry(). 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

busstop_hex_mpsz_df <- busstop_hex_mpsz %>%
  st_drop_geometry()

glimpse(busstop_hex_mpsz_df)
```
With this dataframe, we can join it with the odbus_WDMP dataframe, which contains the passenger volume data. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

od_data <- left_join(odbus_WDMP, busstop_hex_mpsz_df,
                     by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_SZ = SUBZONE_C,
         ORIGIN_SZN = SUBZONE_N,
         DESTIN_BS = DESTINATION_PT_CODE,
         ORIGIN_BIZ_COUNT = BIZ_COUNT,
         ORIGIN_FINSERV_COUNT = FINSERV_COUNT,
         ORIGIN_MRTEXIT_COUNT = MRTEXIT_COUNT,
         ORIGIN_BUSSTOP_COUNT = BUSSTOPS_COUNT,
         ORIGIN_HDBCOMMERCIAL_COUNT = HDBCOMMERCIAL_COUNT,
         ORIGIN_HDBRESIDENTIAL_COUNT = HDBRESIDENTIAL_COUNT,
         ORIGIN_SCHOOLS_COUNT = SCHOOLS_COUNT,
         ORIGIN_LOC_DESC = LOC_DESC,
         ORIGIN_GRID_ID = grid_id)

glimpse(od_data)
```

From the above output, we use the following code chunk to convert ORIGIN_BS into a factor since ORIGIN_BS contains the origin bus stop numbers. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

od_data$ORIGIN_BS <- as.factor(od_data$ORIGIN_BS)
glimpse(od_data)
```


We now check for duplicates in the od_data frame. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

duplicate4 <- od_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

datatable(duplicate4)
```

From the above output, we find that there are 700 duplicate entries. Hence we will use the following code chunk to remove the duplicates. 
```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

od_data <- unique(od_data)

```

We will run the following code chunk to confirm that all duplicates have been removed. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

duplicate5 <- od_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

duplicate5
```

Now we will join the od_data dataframe with the busstop_hex_mpsz_df using the DESTIN_BS. By joining these two dataframes using the Destination Busstops, we can get the counts of the attractiveness/ propelsive factors at the destination. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

od_data <- left_join(od_data , busstop_hex_mpsz_df,
            by = c("DESTIN_BS" = "BUS_STOP_N")) %>%
  rename(DESTIN_SZ = SUBZONE_C,
         DESTIN_SZN = SUBZONE_N,
         DESTIN_BIZ_COUNT = BIZ_COUNT,
         DESTIN_FINSERV_COUNT = FINSERV_COUNT,
         DESTIN_MRTEXIT_COUNT = MRTEXIT_COUNT,
         DESTIN_BUSSTOP_COUNT = BUSSTOPS_COUNT,
         DESTIN_HDBCOMMERCIAL_COUNT = HDBCOMMERCIAL_COUNT,
         DESTIN_HDBRESIDENTIAL_COUNT = HDBRESIDENTIAL_COUNT,
         DESTIN_SCHOOLS_COUNT = SCHOOLS_COUNT,
         DESTIN_LOC_DESC = LOC_DESC,
         DESTIN_GRID_ID = grid_id)


```

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

glimpse(od_data)

```
Let us convert DESTIN_BS, ORIGIN_GRID_ID and DESTIN_GRID_ID to factors. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
od_data$DESTIN_BS <- as.factor(od_data$DESTIN_BS)
od_data$ORIGIN_GRID_ID <- as.factor(od_data$ORIGIN_GRID_ID)
od_data$DESTIN_GRID_ID <- as.factor(od_data$DESTIN_GRID_ID)
```

We will check for duplicates to ensure that each origin only has 1 line that goes to a particular destination. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
duplicate6 <- od_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

datatable(duplicate6)
```

We use the following code chunk to remove the duplicates. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
od_data <- unique(od_data)

```

We run the following code chunk to ensure that all duplicates have been removed. 
```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
duplicate7 <- od_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

duplicate7
```

Let us take a look at the od_data again before we move on. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
glimpse(od_data)
```

However, we note that the hdb_commerical and hdb residential dataframes have dwelling numbers for each building. Currently we only have the counts of HDB commercial and residential buildings in od_data. The counts of these buildings might not be as useful or representative because a grid can have many low dwelling number HDB residential building. Hence, we will add the total dwelling numbers from hdb commercial and hdb residential into the sgmap dataframe. 

First let us add grid_id to the hdb_commercial_sf so that we can later join it to the sgmap dataframe so that they can be assigned with a grid_id. We will also select the relevant columns that we want using the following code chunk. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hdb_commercial_hex <- st_intersection(hdb_commercial_sf, sgmap) %>%
  st_drop_geometry() %>%
  select(postal, SUBZONE_N, total_dwelling_units, grid_id)

glimpse(hdb_commercial_hex) 
```

Notice that a few postal codes can be in 1 grid, so we will use group_by() to sum up the total dwelling units in each hexagon unit. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hdb_commercial_hex1 <- hdb_commercial_hex %>%
  group_by(grid_id)%>%
  summarise(total_comm_units_grid = sum(total_dwelling_units))

glimpse(hdb_commercial_hex1) 
```

We will convert the grid_id into a factor so that it is the same data type as the grid_id in od_data before joining them together. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

hdb_commercial_hex1$grid_id <- as.factor(hdb_commercial_hex1$grid_id)
```

We use the following code chunk to join od_data and hdb_commercial_hex1 together at the ORIGIN_GRID_ID. This will let us know the number of HDB Residential and Commercial dwellings at the ORIGIN_GRID_ID. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

od_data <- left_join(od_data, hdb_commercial_hex1,
                     by=c("ORIGIN_GRID_ID"= "grid_id"))%>%
  rename(ORIGIN_HDBCOMM_UNITS = total_comm_units_grid)

glimpse(od_data)
```

We use the following code chunk to join od_data and hdb_commercial_hex1 together at the DESTIN_GRID_ID. This will let us know the number of HDB Residential and Commercial dwellings at the DESTIN_GRID_ID.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

od_data <- left_join(od_data, hdb_commercial_hex1,
                     by=c("DESTIN_GRID_ID"= "grid_id"))%>%
  rename(DESTIN_HDBCOMM_UNITS = total_comm_units_grid)

glimpse(od_data)
```

We will do the same for the HDB residential data frame. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hdb_residential_hex <- st_intersection(hdb_residental_sf, sgmap) %>%
  st_drop_geometry() %>%
  select(postal, SUBZONE_N, total_dwelling_units, grid_id)

glimpse(hdb_residential_hex)
```


Notice that a few postal codes can be in 1 grid, so we will use group_by() to sum up the total dwelling units in each hexagon unit. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hdb_residential_hex1 <- hdb_residential_hex %>%
  group_by(grid_id)%>%
  summarise(total_res_units_grid = sum(total_dwelling_units))

glimpse(hdb_residential_hex1)
```


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
hdb_residential_hex1$grid_id <- as.factor(hdb_residential_hex1$grid_id)

od_data <- left_join(od_data, hdb_residential_hex1,
                     by=c("ORIGIN_GRID_ID"= "grid_id"))%>%
  rename(ORIGIN_HDBRES_UNITS = total_res_units_grid)

glimpse(od_data)
```

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
od_data <- left_join(od_data, hdb_residential_hex1,
                     by=c("DESTIN_GRID_ID"= "grid_id"))%>%
  rename(DESTIN_HDBRES_UNITS = total_res_units_grid)

glimpse(od_data)
```

Checking if the dataframe has any duplicate data 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
duplicate8 <- od_data %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()

duplicate8
```


Currently the trips are calculated from the origin busstops, but we want to analyse the commuter flows at hexagon level so we will group by the origin_grid_id and destin_grid_id, sum the trips generated from the bus stops in each hexagon grid, and take the mean of the variables counts because we have counted them at the hexagon level (so we do not want to sum them up)

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
od_data2 <- od_data %>%
  drop_na() %>%
  group_by(ORIGIN_GRID_ID, DESTIN_GRID_ID) %>%
  summarise(MORNING_PEAK = sum(TRIPS),
            TORIGIN_BIZ_COUNT = ORIGIN_BIZ_COUNT,
            TORIGIN_FINSERV_COUNT = ORIGIN_FINSERV_COUNT,
            TORIGIN_MRTEXIT_COUNT = ORIGIN_MRTEXIT_COUNT,
            TORIGIN_BUSSTOP_COUNT = ORIGIN_BUSSTOP_COUNT,
            TORIGIN_HDBCOMMERCIAL_COUNT = ORIGIN_HDBCOMMERCIAL_COUNT,
            TORIGIN_HDBRESIDENTIAL_COUNT = ORIGIN_HDBRESIDENTIAL_COUNT,
            TORIGIN_SCHOOLS_COUNT = ORIGIN_SCHOOLS_COUNT,
            TORIGIN_HDBCOMM_UNITS = ORIGIN_HDBCOMM_UNITS, 
            TORIGIN_HDBRES_UNITS = ORIGIN_HDBRES_UNITS, 
            TDESTIN_BIZ_COUNT = DESTIN_BIZ_COUNT,
            TDESTIN_FINSERV_COUNT = DESTIN_FINSERV_COUNT,
            TDESTIN_MRTEXIT_COUNT = DESTIN_MRTEXIT_COUNT,
            TDESTIN_BUSSTOP_COUNT = DESTIN_BUSSTOP_COUNT,
            TDESTIN_HDBCOMMERCIAL_COUNT = DESTIN_HDBCOMMERCIAL_COUNT,
            TDESTIN_HDBRESIDENTIAL_COUNT = DESTIN_HDBRESIDENTIAL_COUNT,
            TDESTIN_SCHOOLS_COUNT = DESTIN_SCHOOLS_COUNT,
            TDESTIN_HDBCOMM_UNITS = DESTIN_HDBCOMM_UNITS,
            TDESTIN_HDBRES_UNITS = DESTIN_HDBRES_UNITS)

```

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
glimpse(od_data2)
```


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
write_rds(od_data2, "data/rds/od_data.rds")

```


## Geovisualisation of OD flows

As we are not plotting intra-zonal flows, the following code chunk removes the intra-zonal flows. 


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
od_data_interzone <- od_data2[od_data2$ORIGIN_GRID_ID != od_data2$DESTIN_GRID_ID,]

head(od_data_interzone)
```


### Creating desire lines 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
flowline <- od2line(flow=od_data_interzone, 
                    zones = sgmap,
                    zone_code = "grid_id")

glimpse(flowline)
```

Let us take a look at the summary statistics of the morning peak trips using summary(). 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
summary(flowline$MORNING_PEAK)

```
From the above output, we see that the range of morning peak trips is wide. The number of trips can be as low as one and as high as 87,864. We can also deduce that the trips volume is extreme by looking at the difference between mean and median values. The mean is 1826 while the median is 300, meaning that there are few extreme high values (hence causing a relatively high mean) and many extreme low values (causing a relatively low median).  

Let us break he morning peak trips into the various quantiles from 10th to the 100th quantile. 
```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
quantile(flowline$MORNING_PEAK, probs = seq(.1, 1, by =.05))

```
Let us plot out the flowlines from the 95th percentile onwards. 

:::{.panel-tabset} 

## Plot 
```{r}
tm_shape(sgmap) +
  tm_polygons("BUSSTOPS_COUNT") + 
  flowline %>%
  filter(MORNING_PEAK >= 8295) %>%
  tm_shape() + 
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)+
    tm_layout(main.title = "Flow for Passenger Volume at 95th Percentile and Above",
            main.title.size = 0.9,
            main.title.position = "center")


```

## Data Table 

```{r}
flowline_95 <- flowline %>%
  filter(flowline$MORNING_PEAK >= 8295)

datatable(flowline_95)
```
:::

Let us visualise the distribution of the trips from the 95th percentile and above using a histogram.



```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
ggplot(flowline_95, aes(x=MORNING_PEAK))+
  geom_histogram(binwidth = 2000, fill="#69b3a2", color="#e9ecef", alpha=0.9)+
  xlim(8000, 87900)

```
From the above output, we see that there are many 


We will use summary() to look at the summary statistics for the 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
summary(flowline_95$MORNING_PEAK)
```


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

tmap_mode("plot")
tm_shape(sgmap) +
  tm_polygons("BUSSTOPS_COUNT") + 
  flowline %>%
  filter(MORNING_PEAK >= 8295, MORNING_PEAK<11000) %>%
  tm_shape() + 
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)+
    tm_layout(main.title = "Flow for Passenger Volume between 8295 and 11000",
            main.title.size = 0.9,
            main.title.position = "center")

```


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

tm_shape(sgmap) +
  tm_polygons("BUSSTOPS_COUNT") + 
  flowline %>%
  filter(MORNING_PEAK>= 11000, MORNING_PEAK<14000) %>%
  tm_shape() + 
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)+
    tm_layout(main.title = "Flow for Passenger Volume between 11000 and 14000",
            main.title.size = 0.9,
            main.title.position = "center")

```


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

tm_shape(sgmap) +
  tm_polygons("BUSSTOPS_COUNT") + 
  flowline %>%
  filter(MORNING_PEAK >= 14000, MORNING_PEAK<23000) %>%
  tm_shape() + 
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)+
    tm_layout(main.title = "Flow for Passenger Volume between 14000 and 23000",
            main.title.size = 0.9,
            main.title.position = "center")


```


```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

tm_shape(sgmap) +
  tm_polygons("BUSSTOPS_COUNT") + 
  flowline %>%
  filter(MORNING_PEAK >= 23000) %>%
  tm_shape() + 
  tm_lines(lwd = "MORNING_PEAK",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.3)+
    tm_layout(main.title = "Flow for Passenger Volume more than or equals to 23000",
            main.title.size = 0.9,
            main.title.position = "center")


```

## Computing Distance Matrix

In spatial interaction, a distance matrix is a table that shows the distance between pairs of locations. We will compute a distance matrix using the sgmap which is the hexagon grid of mpsz. 

First we will use as.Spatial() to convert sgmap from sf tibble dataframe to Spatial Polygons Dataframe using sp package. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

sgmap_sp <- as(sgmap, "Spatial")
sgmap_sp
```


Then we will use the spDist() function of sp package to compute the Euclidean distance between the centriods of the planning subzones. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

DIST <- spDists(sgmap_sp, 
                longlat = FALSE)

head(DIST, n=c(10,10))

```


### Labelling Column and Row Headers of Distance Matrix 
To label the column and row headers of the distance matrix DIST, we will first create a list sorted according to the distance matric by grid_id. 
```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

hex_grid_id <- sgmap$grid_id
```

Then, we will attach the grid_id to the rows and columns of the distance matrix. 
```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

colnames(DIST) <- paste0(hex_grid_id)
rownames(DIST) <- paste0(hex_grid_id)

head(DIST, n=c(10,10))

```


### Pivoting Distance Value by Grid_ID 
We will then pivot the distance matrix into a long table using the row and column grid_id as shown in the following code chunk. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

distpair <- melt(DIST) %>%
  rename(DISTANCE = value)

head(distpair, 10)

```

Notice that the within zone distance is 0.

### Updating Intra-zonal Distances 

To append a constant value to replace the intra-zonal distance of 0, we will select and find out the minimum value of the distance using summary(). 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
distpair %>%
  filter(DISTANCE > 0) %>%
  summary()
```

Next, we add a constant value of 300m, which is less than the minimum distance of 750m (from the above output) into intra-zones distance. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
distpair$DISTANCE <- ifelse(distpair$DISTANCE == 0,
                            300, distpair$DISTANCE)

distpair %>%
  summary()
```

We use the following code chunk to rename the origin and destination fields. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
distpair <- distpair %>%
  rename(orig = Var1,
         dest = Var2)
```

We will save the dataframe for future use. 
```{r}

write_rds(distpair, "data/rds/distpair.rds")
```

## Preparing Flow Data

We will compute the total passenger trip between and within grid_ids by using the code chunk below. The output will have both intra and inter-grid flow data.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
flow_data <- od_data2 %>%
  rename(TRIPS = MORNING_PEAK )

head(flow_data, 10)
```


Code chunk below is used to add three new fields in flow_data dataframe.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
flow_data$FlowNoIntra <- ifelse(
  flow_data$ORIGIN_GRID_ID == flow_data$DESTIN_GRID_ID, 
  0, flow_data$TRIPS)
flow_data$offset <- ifelse(
  flow_data$ORIGIN_GRID_ID == flow_data$DESTIN_GRID_ID, 
  0.000001, 1)

glimpse(flow_data)
```

Before we join the flow_data and dispair data, let us take a look at the distpair data. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
glimpse(distpair)
```

We need to convert the data value type of orgi and dest fields of distpair dataframe into factor data type. Otherwise, we would not be able to perform the left join. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
distpair$orig <- as.factor(distpair$orig)
distpair$dest <- as.factor(distpair$dest)

glimpse(distpair)
```

Now, left_join() of dplyr will be used to flow_data dataframe and distPair dataframe. The output is called flow_data1.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
flow_data1 <- flow_data %>%
  left_join (distpair,
             by = c("ORIGIN_GRID_ID" = "orig",
                    "DESTIN_GRID_ID" = "dest"))
glimpse(flow_data1)
```

We will filter out those intra-zonal flows using the FlowNoIntra column. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
inter_zonal_flow <- flow_data1 %>% 
  filter(FlowNoIntra > 0)

glimpse(inter_zonal_flow)
```


Since Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.

In the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in inter_zonal_flow data frame.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
summary(inter_zonal_flow)
```

The print report above reveals that variables TORIGIN_BIZ_COUNT, TORIGIN_FINSERV_COUNT, TORIGIN_MRTEXIT_COUNT, TORIGIN_SCHOOLS_COUNT,
TORIGIN_HDBCOMM_UNITS, TDESTIN_BIZ_COUNT,  TDESTIN_FINSERV_COUNT, TDESTIN_MRTEXIT_COUNT,TDESTIN_SCHOOLS_COUNT, TDESTIN_HDBCOMM_UNITS consist of 0 values.

We will use the following code chunk to replace 0 values with 0.99. 

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
inter_zonal_flow$TORIGIN_BIZ_COUNT <- ifelse(
  inter_zonal_flow$TORIGIN_BIZ_COUNT == 0,
  0.99, inter_zonal_flow$TORIGIN_BIZ_COUNT)

inter_zonal_flow$TORIGIN_FINSERV_COUNT <- ifelse(
  inter_zonal_flow$TORIGIN_FINSERV_COUNT == 0,
  0.99, inter_zonal_flow$TORIGIN_FINSERV_COUNT)

inter_zonal_flow$TORIGIN_MRTEXIT_COUNT <- ifelse(
  inter_zonal_flow$TORIGIN_MRTEXIT_COUNT == 0,
  0.99, inter_zonal_flow$TORIGIN_MRTEXIT_COUNT)

inter_zonal_flow$TORIGIN_BUSSTOP_COUNT <- ifelse(
  inter_zonal_flow$TORIGIN_BUSSTOP_COUNT == 0, 
  0.99, inter_zonal_flow$TORIGIN_BUSSTOP_COUNT)

inter_zonal_flow$TORIGIN_HDBCOMMERCIAL_COUNT <- ifelse(
  inter_zonal_flow$TORIGIN_HDBCOMMERCIAL_COUNT == 0, 
  0.99, inter_zonal_flow$TORIGIN_HDBCOMMERCIAL_COUNT)

inter_zonal_flow$TORIGIN_HDBRESIDENTIAL_COUNT <- ifelse(
  inter_zonal_flow$TORIGIN_HDBRESIDENTIAL_COUNT == 0,
  0.99, inter_zonal_flow$TORIGIN_HDBRESIDENTIAL_COUNT)

inter_zonal_flow$TORIGIN_SCHOOLS_COUNT <- ifelse(
  inter_zonal_flow$TORIGIN_SCHOOLS_COUNT == 0,
  0.99, inter_zonal_flow$TORIGIN_SCHOOLS_COUNT)

inter_zonal_flow$TORIGIN_HDBCOMM_UNITS <- ifelse(
  inter_zonal_flow$TORIGIN_HDBCOMM_UNITS == 0,
  0.99, inter_zonal_flow$TORIGIN_HDBCOMM_UNITS)

inter_zonal_flow$TORIGIN_HDBRES_UNITS <- ifelse(
  inter_zonal_flow$TORIGIN_HDBRES_UNITS == 0, 
  0.99, inter_zonal_flow$TORIGIN_HDBRES_UNITS)

inter_zonal_flow$TDESTIN_BIZ_COUNT <- ifelse(
  inter_zonal_flow$TDESTIN_BIZ_COUNT == 0,
  0.99, inter_zonal_flow$TDESTIN_BIZ_COUNT)

inter_zonal_flow$TDESTIN_FINSERV_COUNT <- ifelse(
  inter_zonal_flow$TDESTIN_FINSERV_COUNT == 0,
  0.99, inter_zonal_flow$TDESTIN_FINSERV_COUNT)

inter_zonal_flow$TDESTIN_MRTEXIT_COUNT <- ifelse(
  inter_zonal_flow$TDESTIN_MRTEXIT_COUNT == 0,
  0.99, inter_zonal_flow$TDESTIN_MRTEXIT_COUNT)

inter_zonal_flow$TDESTIN_BUSSTOP_COUNT <- ifelse(
  inter_zonal_flow$TDESTIN_BUSSTOP_COUNT == 0,
  0.99, inter_zonal_flow$TDESTIN_BUSSTOP_COUNT)

inter_zonal_flow$TDESTIN_HDBCOMMERCIAL_COUNT <- ifelse(
  inter_zonal_flow$TDESTIN_HDBCOMMERCIAL_COUNT == 0, 
  0.99, inter_zonal_flow$TDESTIN_HDBCOMMERCIAL_COUNT)

inter_zonal_flow$TDESTIN_HDBRESIDENTIAL_COUNT <- ifelse(
  inter_zonal_flow$TDESTIN_HDBRESIDENTIAL_COUNT == 0,
  0.99, inter_zonal_flow$TDESTIN_HDBRESIDENTIAL_COUNT)

inter_zonal_flow$TDESTIN_SCHOOLS_COUNT <- ifelse(
  inter_zonal_flow$TDESTIN_SCHOOLS_COUNT == 0,
  0.99, inter_zonal_flow$TDESTIN_SCHOOLS_COUNT)

inter_zonal_flow$TDESTIN_HDBCOMM_UNITS <- ifelse(
  inter_zonal_flow$TDESTIN_HDBCOMM_UNITS == 0,
  0.99, inter_zonal_flow$TDESTIN_HDBCOMM_UNITS)

inter_zonal_flow$TDESTIN_HDBRES_UNITS <- ifelse(
  inter_zonal_flow$TDESTIN_HDBRES_UNITS == 0,
  0.99, inter_zonal_flow$TDESTIN_HDBRES_UNITS)

```

We run summary() again to confirm that the 0 values have been replaced with 0.99. 
```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
summary(inter_zonal_flow)
```


## Calibrating Spatial Interaction Models


Firstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

ggplot(data=inter_zonal_flow,
       aes(x=TRIPS)) + 
  geom_histogram()

```
From the output, we observe that the distribution is highly skewed and does not resemble a normal distribution. 

Next, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

ggplot(data = inter_zonal_flow,
       aes(x = DISTANCE,
           y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm)

```
Again, their relationship hardly resemble linear relationship.

On the other hand, if we plot the scatter plot by using the log transformed version of both variables using the following code chunk, we can see that their relationship more resembles a linear relationship.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 

ggplot(data = inter_zonal_flow,
       aes(x = log(DISTANCE),
           y = log(TRIPS))) +
  geom_point() +
  geom_smooth(method = lm)

```
For Spatial Interation Model, we will be using log transformed variables so that the relationship resembles a linear relationship and to handle the skewed distributions. We will also be using a poisson regression model because the dependent variable (Trips) cannot be negative. 

Before we proceed to calibrate the spatial interaction models, we will  write a function to calculate R-Squared value as shown below. The R-squared value measure how much variation of the trips can be accounted by the model. 

```{r}
CalcRSquared <- function(observed, estimated){
  r <- cor(observed, estimated)
  R2 <- r^2
  R2
}

```


### Origin (Production) Constrained Spatial Interaction Model 

An origin-constrained spatial interaction model estimates "where they went" using the information on how many people left a particular origin (i.e. origin grid_id in our case). This model can have variables that reflect characteristics of the destinations so it is useful in forecasting destination inflow totals that are unknown. 

In reality, origin-constrained spatial interaction model is useful for policy makers when they come up with new bus routes and bus stops, and estimating how much resource they need to commit during peak hours and non peak hours (e.g. number of double-deck/ single deck buses to deploy during peak and non peak hours respectively). Being able to estimate and project such resources is important because this could help the transport ministry and transport operators determine how many buses and bus drivers they need at any one time.  

The general equation for Origin-constrained Spatial Interaction Model is as follow. 
$$
\lambda{i}{j}= exp(k + \mu{i} + \alpha\ln W{j} - \beta \ln d{i}{j})
$$
We use `glm()` of Base Stats to calibrate an origin-constrained spatial interaction model. the explanatory variables are the destination related variables (such as TDESTIN_BIZ_COUNT, TDESTIN_FINSERV_COUNT) and the distance between the origin and destination (i.e. DISTANCE). 

```{r}
orcSIM_Poisson <- glm(formula = TRIPS ~ 
                ORIGIN_GRID_ID +
                log(TDESTIN_BIZ_COUNT) +
                log(TDESTIN_FINSERV_COUNT) +
                  log(TDESTIN_MRTEXIT_COUNT) +
                  log(TDESTIN_BUSSTOP_COUNT) +
                  log(TDESTIN_HDBCOMMERCIAL_COUNT) +
                  log(TDESTIN_HDBRESIDENTIAL_COUNT) +
                  log(TDESTIN_HDBCOMM_UNITS) +
                  log(TDESTIN_HDBRES_UNITS) + 
                  log(TDESTIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(orcSIM_Poisson)

```


Based on the above output, all the destination related variables are significant because their p-values are less than 0.05. Based on the coefficients, the destination's MRT Exits seemed to  have the biggest influence on the spatial interaction as compared to other variables because it has the biggest coefficient (0.680174) amongst the other variables. The destination's schools has the next highest coefficient of 0.330944. Distance has a negative coefficient of -1.474483. It means that as distance increased by log(l.474483), the interactions decreased. From the output, we observed that the destination HDB Residential Count has a positive coefficient of 0.128855 but the destination HDB Residential Dwelling units has a negative coefficient of -0.150316. It could mean that with HDB residential dwelling units increase, the number of interactions decrease (i.e. less commuter flows).  We will calibrate the model in the next section by removing some variables. 


We compute the R-squared of the Origin Constrained Spatial Interaction Model using the CalcRSquared function written earlier.  
```{r}

CalcRSquared(orcSIM_Poisson$data$TRIPS, orcSIM_Poisson$fitted.values)
```
The above output indicates that these variables explain for 62.65569% of the observed spatial interaction patterns.



### Destination (Attraction) Constrained Spatial Interaction Model 

We will now develop a Destination-constrained Spatial Interaction Model so that we can compare it with the Origin-constrained Spatial Interaction Model. The Destination-constrained Spatial Interaction Model can be used to forecast total outflows from origins. Such a situation could arise, for example, in forecasting the effects of locating a new industrial park in a certain region of Singapore. The number of people to be employed and the businesses to set up are known, the destination-constrained spatial interaction model can be used to forecast the demand for housing in the region that will result from the new employment. 

The general equation for destination constrained spatial interaction model is as follow: 

$$
\lambda{i}{j}= exp(k + \mu\ln V{i} + \alpha{i} - \beta \ln d{i}{j})
$$

```{r}
decSIM_Poisson <- glm(formula = TRIPS ~ 
                DESTIN_GRID_ID +
                log(TORIGIN_BIZ_COUNT) +
                log(TORIGIN_FINSERV_COUNT) +
                  log(TORIGIN_MRTEXIT_COUNT) +
                  log(TORIGIN_BUSSTOP_COUNT) +
                  log(TORIGIN_HDBCOMMERCIAL_COUNT) +
                  log(TORIGIN_HDBRESIDENTIAL_COUNT) +
                  log(TORIGIN_HDBCOMM_UNITS) +
                  log(TORIGIN_HDBRES_UNITS) + 
                  log(TORIGIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(decSIM_Poisson)

```

```{r}

CalcRSquared(decSIM_Poisson$data$TRIPS, decSIM_Poisson$fitted.values)
```


### Doubly constrained Model



The general formula of Doubly Constrained Spatial Interaction Model

$$
\lambda{i}{j}= exp(k + \mu{i} + \alpha{i} - \beta \ln d{i}{j})
$$

```{r}
dbcSIM_Poisson <- glm(formula = TRIPS ~ 
                ORIGIN_GRID_ID + 
                DESTIN_GRID_ID +
                log(DISTANCE),
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(dbcSIM_Poisson)

```


```{r}
CalcRSquared(dbcSIM_Poisson$data$TRIPS, dbcSIM_Poisson$fitted.values)

```


## Comparing Models 

```{r}
model_list <- list(originConstrained=orcSIM_Poisson,
                   destinationConstrained=decSIM_Poisson,
                   doublyConstrained=dbcSIM_Poisson)

```



```{r}

compare_performance(model_list,
                    metrics = "RMSE")

```

```{r}
df <- as.data.frame(orcSIM_Poisson$fitted.values) %>%
  round(digits = 0)

```



```{r}
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df) %>%
  rename(orcTRIPS = "orcSIM_Poisson$fitted.values")

```


```{r}
df2 <- as.data.frame(decSIM_Poisson$fitted.values) %>%
  round(digits = 0)

```

```{r}
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df2) %>%
  rename(decTRIPS = "decSIM_Poisson$fitted.values")

```


```{r}
df3 <- as.data.frame(dbcSIM_Poisson$fitted.values) %>%
  round(digits = 0)

```


```{r}
inter_zonal_flow <- inter_zonal_flow %>%
  cbind(df3) %>%
  rename(dbcTRIPS = "dbcSIM_Poisson$fitted.values")

```


```{r}

orc_p <- ggplot(data = inter_zonal_flow,
                aes(x = orcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm) +
  coord_cartesian(xlim=c(0,70000),
                  ylim=c(0,70000))

dec_p <- ggplot(data = inter_zonal_flow,
                aes(x = decTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm) +
  coord_cartesian(xlim=c(0,70000),
                  ylim=c(0,70000))

dbc_p <- ggplot(data = inter_zonal_flow,
                aes(x = dbcTRIPS,
                    y = TRIPS)) +
  geom_point() +
  geom_smooth(method = lm) +
  coord_cartesian(xlim=c(0,70000),
                  ylim=c(0,70000))
```


```{r}

ggarrange(orc_p, dec_p,
          dbc_p,
          ncol = 2,
          nrow = 2)
```

### Further Calibration of the Models 

#### Remove certain variables 
```{r}
orcSIM_Poisson2 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID_ID +
                log(TDESTIN_BIZ_COUNT) +
                log(TDESTIN_FINSERV_COUNT) +
                  log(TDESTIN_MRTEXIT_COUNT) +
                  log(TDESTIN_BUSSTOP_COUNT) +
                  log(TDESTIN_HDBCOMM_UNITS) +
                  log(TDESTIN_HDBRES_UNITS) + 
                  log(TDESTIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(orcSIM_Poisson2)

```

```{r}
CalcRSquared(orcSIM_Poisson2$data$TRIPS, orcSIM_Poisson2$fitted.values)

```

```{r}
decSIM_Poisson2 <- glm(formula = TRIPS ~ 
                DESTIN_GRID_ID +
                log(TORIGIN_BIZ_COUNT) +
                log(TORIGIN_FINSERV_COUNT) +
                  log(TORIGIN_MRTEXIT_COUNT) +
                  log(TORIGIN_BUSSTOP_COUNT) +
                  log(TORIGIN_HDBCOMM_UNITS) +
                  log(TORIGIN_HDBRES_UNITS) + 
                  log(TORIGIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow,
              na.action = na.exclude)
summary(decSIM_Poisson2)

```


```{r}
CalcRSquared(decSIM_Poisson2$data$TRIPS, decSIM_Poisson2$fitted.values)

```

```{r}
model_list2 <- list(originConstrained2=orcSIM_Poisson2,
                   destinationConstrained2=decSIM_Poisson2,
                   doublyConstrained=dbcSIM_Poisson)

```



```{r}

compare_performance(model_list2,
                    metrics = "RMSE")

```

#### Remove outliers 

Filter out the 2 extreme trip values: 87864, 71996

```{r}
inter_zonal_flow1 <- inter_zonal_flow %>%
  filter(TRIPS <71996)

summary(inter_zonal_flow1$TRIPS)
```

```{r}
orcSIM_Poisson3 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID_ID +
                log(TDESTIN_BIZ_COUNT) +
                log(TDESTIN_FINSERV_COUNT) +
                  log(TDESTIN_MRTEXIT_COUNT) +
                  log(TDESTIN_BUSSTOP_COUNT) +
                  log(TDESTIN_HDBCOMMERCIAL_COUNT) +
                  log(TDESTIN_HDBRESIDENTIAL_COUNT) +
                  log(TDESTIN_HDBCOMM_UNITS) +
                  log(TDESTIN_HDBRES_UNITS) + 
                  log(TDESTIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow1,
              na.action = na.exclude)
summary(orcSIM_Poisson3)

```

```{r}
CalcRSquared(orcSIM_Poisson3$data$TRIPS, orcSIM_Poisson3$fitted.values)

```

```{r}
decSIM_Poisson3 <- glm(formula = TRIPS ~ 
                DESTIN_GRID_ID +
                log(TORIGIN_BIZ_COUNT) +
                log(TORIGIN_FINSERV_COUNT) +
                  log(TORIGIN_MRTEXIT_COUNT) +
                  log(TORIGIN_BUSSTOP_COUNT) +
                  log(TORIGIN_HDBCOMMERCIAL_COUNT) +
                  log(TORIGIN_HDBRESIDENTIAL_COUNT) +
                  log(TORIGIN_HDBCOMM_UNITS) +
                  log(TORIGIN_HDBRES_UNITS) + 
                  log(TORIGIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow1,
              na.action = na.exclude)
summary(decSIM_Poisson3)

```

```{r}
CalcRSquared(decSIM_Poisson3$data$TRIPS, decSIM_Poisson3$fitted.values)

```



```{r}
dbcSIM_Poisson2 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID_ID + 
                DESTIN_GRID_ID +
                log(DISTANCE),
              family = poisson(link = "log"),
              data = inter_zonal_flow1,
              na.action = na.exclude)
summary(dbcSIM_Poisson2)

```


```{r}
CalcRSquared(dbcSIM_Poisson2$data$TRIPS, dbcSIM_Poisson2$fitted.values)

```


```{r}
model_list3 <- list(originConstrained3=orcSIM_Poisson3,
                   destinationConstrained3=decSIM_Poisson3,
                   doublyConstrained2=dbcSIM_Poisson2)

```



```{r}

compare_performance(model_list3,
                    metrics = "RMSE")

```


#### Removing both the variables and outliers 

```{r}
orcSIM_Poisson4 <- glm(formula = TRIPS ~ 
                ORIGIN_GRID_ID +
                log(TDESTIN_BIZ_COUNT) +
                log(TDESTIN_FINSERV_COUNT) +
                  log(TDESTIN_MRTEXIT_COUNT) +
                  log(TDESTIN_BUSSTOP_COUNT) +
                  log(TDESTIN_HDBCOMM_UNITS) +
                  log(TDESTIN_HDBRES_UNITS) + 
                  log(TDESTIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow1,
              na.action = na.exclude)
summary(orcSIM_Poisson4)

```

```{r}
CalcRSquared(orcSIM_Poisson4$data$TRIPS, orcSIM_Poisson4$fitted.values)

```

```{r}
decSIM_Poisson4 <- glm(formula = TRIPS ~ 
                DESTIN_GRID_ID +
                log(TORIGIN_BIZ_COUNT) +
                log(TORIGIN_FINSERV_COUNT) +
                  log(TORIGIN_MRTEXIT_COUNT) +
                  log(TORIGIN_BUSSTOP_COUNT) +
                  log(TORIGIN_HDBCOMM_UNITS) +
                  log(TORIGIN_HDBRES_UNITS) + 
                  log(TORIGIN_SCHOOLS_COUNT) + 
                log(DISTANCE) - 1,
              family = poisson(link = "log"),
              data = inter_zonal_flow1,
              na.action = na.exclude)
summary(decSIM_Poisson4)

```

```{r}
CalcRSquared(decSIM_Poisson4$data$TRIPS, decSIM_Poisson4$fitted.values)

```


```{r}
model_list4 <- list(originConstrained4=orcSIM_Poisson4,
                   destinationConstrained4=decSIM_Poisson4,
                   doublyConstrained2=dbcSIM_Poisson2)

```



```{r}

compare_performance(model_list4,
                    metrics = "RMSE")

```

### Final Model 



## Conclusion and Future Work 




