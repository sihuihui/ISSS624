---
title: "In-Class Exercise 2C: Emerging Hot Spot Analysis"
subtitle: "Using sfdep package" 
author: "Goh Si Hui"
date: 2023/11/25
date-format: long
date-modified: "last-modified"
format: html 
execute: 
  echo: true
  eval: true
  warning: false
editor: visual 
---

## Overview

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:

-   Building a space-time cube,

-   Calculating Getis-Ord local Gi\* statistic for each bin by using an FDR correction,

-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test,

-   Categorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.

## Getting Started

We will first load the necessary packages using the following code chunk:

-   **tmap**: for thematic mapping
-   **sf**: for geospatial data handling (e.g. importing and exporting for spatial data and geoprocessing)
-   **tidyverse**: a family of R packages for non-spatial data handling
-   **knitr**: to generate static html tables
-   **sfdep**: to calculate spatial weights and matrices, space time cube and hot spot analysis
-   **plotly**: to make the graphs interactive

```{r}
pacman::p_load(tmap, sf, tidyverse, sfdep, knitr, plotly)
```

## Preparing the Geospatial Data

### Importing the data

```{r}
hunan <- st_read(dsn = "data/geospatial", layer="Hunan")
```

From the above outcome, we know that `hunan`data is simple feature (sf) data frame with 88 features (each representing 1 geographical entity), and each feature is a polygon.It uses the WGS84 projection.

## Preparing the Aspatial Data

### Importing the data

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

Let us take a look at the GDPPC data.

```{r}

kable(head(GDPPC))

```

## Creating a Time Series Cube

We use `spacetime()` of sfdep to create a space time cube.

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                 .loc_col = "County",
                 .time_col = "Year")
```

::: callout-note
Note that the "time" variable needs to be in 'int' or 'num' format when using `spacetime()`.
:::

We can use `is_spacetime_cube()` of **sfedep** package to verify if GDPPC_st is indeed a space time cube object.

```{r}
is_spacetime_cube(GDPPC_st)
```

The TRUE return confirms that GDPPC_st object is indeed an space time cube.

## Computing Gi\*

First we will compute an inverse distance weight matrix.

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>% 
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry, scale = 1,
                                  alpha=1),
         .before = 1) %>%
  set_nbs("nb") %>% 
  set_wts("wt")
```

::: callout-note
-   activate() of dplyr package is used to activate the geometry context
-   mutate() of dplyr package is used to create 2 columns nb and wt.
-   Then we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts().
    -   Note that row order is very important so do not rearrange the observations after using set_nbs() or set_wts().
:::

The following output shows that this dataset now has neighbours and weighrs for each time-slice

```{r}
head(GDPPC_nb)

```

We can now use these new columns to calculate the local Gi\* for each location. We can do this by grouping Year and using `local_gstar_perm()` of **sfdep** package. After which, we use `unnest()` to unnest gi_star column of the newly created gi_stars data frame.

```{r}

gi_stars <- GDPPC_nb %>% 
  group_by(Year) %>% 
  mutate(gi_star = local_gstar_perm(GDPPC, nb, wt)) %>% 
  tidyr::unnest(gi_star)

```

## Mann-Kendall Test

With these Gi\* measures, we can then evaluate each location for a trend using the Mann-Kendall test. The following code chunk using Changsha county.

```{r}
cbg <- gi_stars %>% 
  ungroup() %>% 
  filter(County == "Changsha") |>
  select(County, Year, gi_star)
```

Next, we plot the result using ggplot2 functions.

```{r}
ggplot(data = cbg, 
       aes(x = Year, y = gi_star)) +
  geom_line() +
  theme_light() 
```

```{r}
p <- ggplot(data = cbg, 
       aes(x = Year, y = gi_star)) +
  geom_line() +
  theme_light() 

ggplotly(p)
```

```{r}
cbg %>% 
  summarise(mk = list(
    unclass(Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)

```

In the above result, sl is the p-value, which is 0.007416964. This result tells us that there is a slight upward but insignificant trend.

We can replicate this for each location by using `group_by()` of dplyr package.

```{r}
ehsa <- gi_stars %>%
  group_by(County) %>%
  summarise(mk = list(unclass(Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)

```

## Arrange to show significant emerging hot/cold spots

Now we will arrange to share the significant emerging hot/cold spots using the following code chunk.

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)
```

## Performing Emerging Hotspot Analysis

We will now perform EHSA analysing using `emerging_hotspot_analysis()` of **sfdep** package. It takes a spacetime object (i.e., GDPPC_st)and the variable of interest (i.e., GDPPC) for .var arugment. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.

```{r}
ehsa <- emerging_hotspot_analysis(
  x = GDPPC_st,
  .var = "GDPPC",
  k = 1,
  nsim = 99
)

```

## Visualising the distribution of EHSA classes

We use ggplot2 functions to plot the distribution of EHSA classes as a bar chart.

```{r}

ggplot(data = ehsa, 
       aes(x=classification)) + geom_bar()

```

The figure above shows that sporadic cold spots class has the highest number of county.

## Visualising EHSA

Before we visualise the geographic distribution of EHSA classes, we need to join both `hunan` and `ehsa` data together using the following code chunk.

```{r}
hunan_ehsa <- hunan %>%
  left_join(ehsa,
            by = join_by(County == location))

```

Then, we will use tmap functions to plot a categorical choropleth map using the following code chunk.

```{r}
ehsa_sig <- hunan_ehsa %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(hunan_ehsa) + 
  tm_polygons() +
  tm_borders(alpha = 0.5) + 
  tm_shape(ehsa_sig)+
  tm_fill("classification") +
  tm_borders(alpha = 0.4)

```
