---
title: "Hands-on Exercise 2B: Global and Local Measures of Spatial Autocorrelations"
author: "Goh Si Hui"
date: 2023/11/24
date-format: long
date-modified: "last-modified"
format: html 
execute: 
  echo: true
  eval: true
  warning: false
editor: visual 
---

## Overview

In this hands-on exercise, we will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) using the **spdep** package.

## The Analytical Question

In spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be "is there sign of spatial clustering?". And, if the answer for this question is yes, then our next question will be "where are these clusters?"

In this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of [Hunan Province](https://en.wikipedia.org/wiki/Hunan "About Hunan"), People's Republic of China.

## Getting Started

### Packages

First, we will import the relevant packages that we will be using for this hands-on exercise.

```{r}
pacman::p_load(sf,spdep,tmap,tidyverse,knitr)
```

We will be using the following packages:

-   **sf**: to import and handle geospatial data,

-   **tidyverse**: to handle and wrangle attribute data,

-   **knitr**: to generate tables for matrices,

-   **spdep**: to compute spatial weights, global and local spatial autocorrelation statistics; and

-   **tmap**: to prepare and plot cartographic quality chropleth map.

### Importing Data

The datasets used in this hands-on exercise are:

-   `Hunan county boundary layer:` a geospatial data set in ESRI shapefile format

-   `Hunan_2012.csv`: an aspatial data set in csv format. It contains selected Hunan's local development indicators in 2012.

::: callout-note
The datasets from this exercise were provided as part of the coursework and downloaded from the student learning portal.
:::

#### Geospatial Data

First, we will use `st_read()` of **sf** package to import `Hunan county boundary layer` (a shapefile) into R.

::: panel-tabset
## Codes

```{r}
#| eval: false
hunan <- st_read(dsn = "data/geospatial", layer = "Hunan")

```

## Output

```{r}
#| echo: false
hunan <- st_read(dsn = "data/geospatial", layer = "Hunan")

```

## Data

```{r}
glimpse(hunan)
```
:::

From the output, we know that `hunan` is a polygon sf dataframe with 88 features and 7 fields. It also uses a WGS84 geometric coordinates system.

#### Aspatial Data

We will import `Hunan_2012.csv` into R using `read_csv()` of **readr** package.

::: panel-tabset
## Codes

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

## Data

```{r}
glimpse(hunan2012)
```
:::

### Performing Relational Join

We will update the attribute table of `hunan`'s spatial polygons dataframe with the attribute fields of `hunan2012` dataframe using the `left_join()` of **dplyr** package.

::: panel-tabset
## Codes

```{r}
hunan_joined <- left_join(hunan, hunan2012,
                   by="County") 
```

## Output

```{r}
kable(head(hunan_joined))
```
:::

As we intend to only show the distribution of Gross Domestic Product Per Capita (GDPPC), we can drop some of the columns that we will not be using by selecting the columns that we want using `select()`.

::: panel-tabset
## Codes

```{r}
hunans <- hunan_joined %>% 
  select(c(1:4, 7, 15)) 
```

## Output

```{r}
kable(head(hunans))
```
:::

## Visualising Regional Development Indicator

We will show the distribution of Gross Domestic Product per Capita (GDPPC) 2012 using `qtm()` of **tmap** package using the following code chunk.

::: panel-tabset
## Codes

```{r}
#| eval: false

equal <- tm_shape(hunans) +
  tm_fill("GDPPC",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(hunans) +
  tm_fill("GDPPC",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)

```

## Visualisation

```{r}
#| echo: false


equal <- tm_shape(hunans) +
  tm_fill("GDPPC",
          n = 5,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval classification")

quantile <- tm_shape(hunans) +
  tm_fill("GDPPC",
          n = 5,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile classification")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)

```
:::

## Global Spatial Autocorrelation

In this section, we will learn how to compute global spatial autocorrelation statistics and perform spatial complete randomness test for global spatial autocorrelation.

### Computing Contiguity Spatial Weights

Before we can compute the global spatial autocorrelation statistics, we need to construct spatial weights of the study area. Spatial weights is used to define the neighbourhood relationships between the geographical units (i.e., county) in the study area.

::: callout-note
To learn more about computing spatial weights, please refer to [Hands-on Exercise 2a](https://gohsihui.netlify.app/hands-on_ex/hands-on_ex2/hands-on_ex2a "Hands-on Ex2a").
:::

In the code chunk below, `poly2nb()` of **spdep** package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.

As mentioned in [Hands-on Exercise 2a](https://gohsihui.netlify.app/hands-on_ex/hands-on_ex2/hands-on_ex2a "Hands-on Ex2a"), `poly2nb()`'s default argument for `Queen` is `queen=TRUE`, meaning that the function computes Queen contiguity by default. If we want to compute Rook contiguity, we need to set `queen=FALSE`.

We use the following code chunk to compute Queen contiguity weight matrix.

```{r}
wm_q <- poly2nb(hunans, queen = TRUE)
summary(wm_q)
```

The summary report above shows that there are 88 regions in Hunan. The most connected region has 11 neighbours and there are 2 regions with only 1 neighbours. On average, each region has 5.090909 neighbours.

### Row-standardised Weight Matrix

Next, we need to assign weights to each neighbouring polygon. We will assign each neighbouring polygon with equal weight (`style="W"`). This is accomplished by assigning the fraction 1/(total number of neighbours) to each neighbouring county then summing the weighted income values.

While assigning each neighbouring polygon with the same weight is most intuitive way to summarise the neighbours' values, polygons which are situated along the edges of the map will base their lagged values on fewer polygons (due to the nature of their positions on the map). This could cause potential over- or under- estimation of the true nature of the spatial autocorrelation in the data.

For the purpose of this hands-on exercise, we will use the `style="W"` option for simplicity sake.

::: callout-note
The **`nb2listw()`** function can take in the following styles:

-   B is the basic binary coding

-   W is row standardised (sums over all links to n)

-   C is globally standardised (sums over all links to n)

-   U is equal to C divided by the number of neighbours (sums over all links to unity)

-   S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999

-   minmax is based on Kelejian and Prucha (2010), and divides the weights by the minimum of the maximum row sums and maximum column sums of the input weights. It is similar to the C and U styles.
:::

```{r}
rswm_q <- nb2listw(wm_q, style="W", zero.policy = TRUE)
rswm_q
```

::: callout-warning
The `zero.policy=TRUE` option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a `zero.policy = FALSE` would return an error.
:::

### Global Spatial Autocorrelation: Moran's I

In this section, we will learn how to perform Moran's I statistics testing using `moran.test()` of **spdep** package.

::: callout-Note
Moran's test for spatial autocorrelation using a spatial weights matrix in weights list form. The assumptions underlying the test are sensitive to the form of the graph of neighbour relationships and other factors, and results may be checked against those of `moran.mc` permutations.
:::

#### Moran's I test

The code chunk below performs Moran's I statistical testing using `moran.test()` of **spdep** package.

```{r}
moran.test(hunans$GDPPC, 
           listw = rswm_q,
          zero.policy = TRUE,
          alternative = "greater",
          na.action = na.omit)
```

The Moran I test is a measure of spatial autocorrelation, which assesses whether the observed pattern in the spatial distribution of a variable is different from what would be expected under spatial randomness.

From the above outcome, we see that the Moran I statistic is 0.30075, and its standard deviation is 4.7351. The p-value is 1.095e-06, which is very small. Assuming that our chosen significance level is 0.05, we would reject the null hypothesis since p-value \< 0.05, suggesting strong evidence against the null hypothesis of spatial randomness.

::: callout-Note
The alternative hypothesis is "greater", indicating that we are testing if there is a positive spatial autocorrelation (i.e., similar values are close to each other). We can specify the alternative hypothesis using `alternative =`. The default value is `"greater"`, but it can be changed to `"less"` or `"two.sided"`.
:::

Hence, the results suggests that there is a significant positive spatial autocorrelation in the variable **GDPPC** and the observed spatial pattern is not likely to have occurred by random chance.

#### Computing Monte Carlo Moran's I

If we doubt that the assumptions of Moran's I are true (normality and randomisation), we can use a Monte Carlo simulation. The purpose of the Monte Carlo simulation is to estimate the significance of the Moran I statistic through random permutations. We will: - Simulate Moran's I n times under the assumption of no spatial pattern, - Assign all regions the mean value, - Calculate Moran's I, - Compare the actual value of Moran's I to randomly simulated distribution to obtain p-value (pseudo significance).

The code chunk below performs permutation test for Moran's I statistic using `moran.mc()` of **spdep** package. A total of 1000 simulation will be performed.

```{r}
set.seed(1234)
bperm = moran.mc(hunans$GDPPC,
                 listw = rswm_q,
                 nsim = 999,
                 zero.policy = TRUE,
                 na.action = na.omit)
```

::: callout-note
The simulation was run with 999 permutations (`nsim = 999`) plus the observed statistic, making a total of 1000 simulations.
:::

The Monte Carlo simulation supports the earlier findings from the Moran I test. The small p-value (0.001) indicates that the observe spatial pattern in the variable **GDPPC** is unlikely due to random chance and there is a strong evidence of positive spatial autocorrelation.

#### Visualising Monte Carlo Moran's I

It is a good practice for us the examine the simulated Moran's I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.

```{r}
mean(bperm$res[1:999])
```

```{r}
var(bperm$res[1:999])
```

```{r}
summary(bperm$res[1:999])

```

We will use hist() and abline() of R Graphics to plot the histogram.

```{r}
hist(bperm$res,
     freq=TRUE,
     breaks = 20,
     xlab = "Simulated Moran's I")

abline(v=0,
       col="red")

```

The observed statistic from Monte Carlo Moran's I Simulation is 0.300749970, which falls way to the right of the histogram distribution suggesting that **GDPPC** values are clustered (a positive Moran's I value suggests clustering while a negative Moran'sI value suggests dispersion).

### Global Spatial Autocorrelation: Geary's C

In this section, we will learn how to perform Geary's c statistics testing by using appropriate functions of spdep package.

#### Geary's C test

The code chunk below performs Geary's C test for spatial autocorrelation by using `geary.test()` of **spdep** package.

```{r}
geary.test(hunans$GDPPC, listw = rswm_q)
```

The p-value associated with the Geary C test is 0.0001526, which is very small and less than 0.05. This means that we can reject the null hypothesis of spatial randomness.

::: callout-Note
The alternative hypothesis's default value is "Expectation greater than statistic", indicating that we are testing whether the expected value of Geary's C is greater than the observed statistic. This suggests positive spatial autocorrelation.
:::

The Geary's C test result suggests that there is significant positive spatial autocorrelation in the variable **GDPPC** in the `hunans` dataset based on the specified spatial weights matrix. The observed spatial pattern is not likely to have occurred by random chance.

#### Computing Monte Carlo Geary's C

The code chunk below performs permutation test for Geary's C statistuc using `geary.mc()` of **spdep** package.

```{r}
set.seed(1234)
bperm_g = geary.mc(hunans$GDPPC,
                 listw = rswm_q,
                 nsim = 999)
bperm
```

#### Visualising Monte Carlo Geary's C

We will plot a histogram to reveal the distribution of the simulated values using the following code chunks.

```{r}
mean(bperm_g$res[1:999])
```

```{r}
var(bperm_g$res[1:999])
```

```{r}
summary(bperm_g$res[1:999])
```

```{r}
hist(bperm_g$res, freq = TRUE, breaks = 20, xlab="Simulated Geary C")
abline(v=1, col = "red")
```

## Spatial Correlogram

Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran's I or Geary's c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.

### Compute Moran's I Correlogram

We will use `sp.correlogram()` of **spdep package** to compute a 6-lag spatial correlogram of GDPPC for Moran's I (`method = "I"`).We will then use the plot() function of base Graph to plot the output.

```{r}
MI_corr <- sp.correlogram(wm_q, hunans$GDPPC,
                          order = 6, method = "I",
                          style = "W")

plot(MI_corr)

```

Plotting the output might not allow us to provide complete interpretation because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.

```{r}
print(MI_corr)
```

### Compute Geary's C Correlogram and Plot

We will use `sp.correlogram()` of **spdep package** to compute a 6-lag spatial correlogram of GDPPC for Geary's C (`method = "C"`).We will then use the plot() function of base Graph to plot the output.

```{r}
GC_corr <- sp.correlogram(wm_q,
                          hunans$GDPPC,
                          order = 6,
                          method = "C",
                          style = "W")

plot(GC_corr)

```

Similar to the previous step, we will print out the analysis report by using the code chunk below.

```{r}
print(GC_corr)
```

## Cluster and Outlier Analysis

Local Indicators of Spatial Association (LISA) are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance, if we are studying cancer rates among census tracts in a given city, local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.

In this section, we will learn how to apply appropriate LISA, especially local Moran'I, to detect clusters and/or outliers from GDPPC of Hunan Province.

### Computing Local Moran's I

To compute local Moran's I, the `localmoran()` function of **spdep** will be used. `localmoran()` computes *li* values, given a set of *zi* values and a listw object providing neighbouring weights information for the polygon associated with the *zi* values.

The code chunk below computes local Moran's I of GDPPC at the county level.

```{r}
fips <- order(hunans$County)
localMI <- localmoran(hunans$GDPPC, rswm_q)
head(localMI)

```

`localmoran()` function returns a matrix of values with the following columns:

-   *li*: the local Moran's I statistics

-   *E.li*: the expectation of local moran statistic under the randomisation hypothesis

-   *Var.li*: the variance of local moran statistic under the randomisation hypothesis

-   *Z.li*: the standard deviation of local moran statistic

-   *Pr()*: the p-value of local moran statistic

We use `printCoefmat()` to list the content of the local Moran matrix.

::: panel-tabset
##Codes

```{r}
#| eval: false
printCoefmat(data.frame(
  localMI[fips,],
  row.names = hunans$County[fips]),
  check.names=FALSE)
```

##Output

```{r}
#| echo: false
printCoefmat(data.frame(
  localMI[fips,],
  row.names = hunans$County[fips]),
  check.names=FALSE)
```
:::

### Mapping Local Moran's I values and p-values

Before mapping the local Moran's I map, we append the local Moran's I dataframe (`localMI`) onto the Hunan Spatial Polygon Data Frame (`hunans`).

```{r}
hunan.localMI <- cbind(hunans,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

We then use choropleth mapping functions of **tmap** package to plot the local Moran's I values using the following code chunk:

```{r}
tm_shape(hunan.localMI) + 
  tm_fill(col= "Ii",
          style = "pretty",
          palette = "RdBu",
          title = "Local Moran Statistics") + 
  tm_borders(alpha = 0.5)

```

The choropleth map shows that there are both positive and negative li values. Hence, we should consider the p-values for each of these values to determine their significance.

The following code chunk creates a choropleth map of Moran's I p-values using **tmap** package.

```{r}
tm_shape(hunan.localMI) + 
  tm_fill(col= "Pr.Ii",
          breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette = "-Blues",
          title = "Local Moran's I p-values") + 
  tm_borders(alpha = 0.5)

```

For effective interpretation, we plot both the local Moran's I values map and its corresponding p-values map next to each other.

```{r}
localMI.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "Local Moran Statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)

```

## Creating a LISA Cluster Map

The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. Before we generate the LISA cluster map, we would need to plot the Moran Scatterplot.

###Plotting Moran Scatterplot The Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.

```{r}
nci <- moran.plot(hunans$GDPPC, rswm_q, 
                  labels=as.character(hunans$County),
                  xlab="GDPPC 2012",
                  ylab="Spatially Lag GDPPC 2012")

```

Notice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC.

### Plotting Moran Scatterplot with Standardised Variable

We will use `scale()` to center and scale the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.

```{r}
hunans$Z.GDPPC <- scale(hunans$GDPPC) %>%
  as.vector()
```

The `as.vector()` added to the end if to make sure that the data type we get from this vector maps neatly onto our dataframe.

We now plot the Moran scatterplot with standardised variable using the following code chunk.

```{r}
nci2 <- moran.plot(hunans$Z.GDPPC, rswm_q,
                   labels = as.character((hunans$County),                   xlab= "z-GDPPC 2012",                            ylab="Spatially Lag z-GDPPC 2012"))

```

### Preparing LISA Map Classes

First we generate the quadrants of the LISA cluster map.

```{r}
quadrant <-vector(mode="numeric", length=nrow(localMI))

```

Next, we derive the spatially legged variable of interest (i.e., GDPPC) and centers the spatially lagged variable around its mean.

```{r}
hunans$lag_GDPPC <- lag.listw(rswm_q, hunans$GDPPC)

DV <- hunan$lag_GDPPC - mean(hunans$lag_GDPPC)
```

Then, we center the local Moran's around the mean.

```{r}

LM_I <- localMI[,1] - mean(localMI[,1])
```

Next, we will set a statistical significance level for the local Moran.

```{r}
signif <- 0.05
```

We also define the low-low(1), low-high(2), high-low(3) and high-high(4) categories.

```{r}
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4  

```

Lastly,we place non-significant Moran in the category 0.

```{r}
quadrant[localMI[,5]>signif] <- 0
```

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
hunans$lag_GDPPC <- lag.listw(rswm_q, hunans$GDPPC)
DV <- hunans$lag_GDPPC - mean(hunans$lag_GDPPC)     
LM_I <- localMI[,1]   
signif <- 0.05       
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4    
quadrant[localMI[,5]>signif] <- 0
```

### Plotting LISA Map

Now, we can build the LISA map by using the following code chunk.

```{r}
hunan.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

tm_shape(hunan.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

```

For effective interpretation,we plot both the local Moran's I values map and its corresponding p-values map next to each other.

```{r}
gdppc <- qtm(hunans, "GDPPC")

hunan.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(hunan.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

tmap_arrange(gdppc, LISAmap, 
             asp=1, ncol=2)
```

We can also include the local Moran's I map and p-value map as shown below for easy comparison.

```{r}
gdppc <- qtm(hunans, "GDPPC")

hunan.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(hunan.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

localMI.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "Local Moran Statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(hunan.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(gdppc,LISAmap,localMI.map, pvalue.map)
```

## Hot Spot and Cold Spot Area Analysis

Besides detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.

The term 'hot spot' has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).

### Getis and Ord's G-Statistics

The Getis and Ord's G-statistics (Getis and Ord, 1972; Ord and Getis, 1995) looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.

The analysis consists of three steps:

-   Deriving spatial weight matrix
-   Computing Gi statistics
-   Mapping Gi statistics

### Deriving Distance-based Weight Matrix

First, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.

There are two type of distance-based proximity matrix, they are:

-   fixed distance weight matrix; and
-   adaptive distance weight matrix.

#### Deriving the centroid

We will need points to associate with each polygon before we can make our connectivity graph. We will need to run `st_centroid()` and also use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length.

To get the longtitude values, we map the st_centroid() function over the geometry column of hunan and access the longitude value through the double bracket notation \[\[\]\] and 1. This allows us to get only the longitude, which is the first value in each centroid.

```{r}
longitude <- map_dbl(hunans$geometry, ~st_centroid(.x)[[1]])

```

To get the latitude, we will use change the "1" in the double bracket notation to "2" since latitude is the second value in each centroid.

```{r}
latitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])

```

Now that we have latitude and longitude, we use cbind() to put longitude and latitude into the same object.

```{r}
coords <- cbind(longitude, latitude)
```

#### Determine the cut-off distance

First, we need to determine the upper limit for distance band using the steps below.

1.  Return a matrix with the indices of points belonging to the set of the k nearest neighbours of each other using knearneigh() of spdep.

2.  Convert the k-nearest neighbour object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids using knn2nb().

3.  Return the length of neighbour relationship edges using nbdists() of spdep. This function returns the Euclidean distances along the links in a list of the same form as the neighbours list. If longlat=TRUE, Great Circle distances are used.

4.  Remove the list structure of the returned object using unlist().

```{r}
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)

```

The summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.

#### Computing the fixed distance weight matrix

We will now compute the distance weight matrix using `dnearneigh()` and the following code chunk.

```{r}
wm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)
wm_d62
```

From the above output, we know that there are 88 regions in Hunan and on average each region has 3.68 neighbours.

Next, `nb2listw()` is used to convert the nb object into spatial weights object.

```{r}
wm62_lw <- nb2listw(wm_d62, style = "B")
summary(wm62_lw)

```

### Computing adaptive distance weight matrix

One of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothens the neighbour relationship across more neighbours.

We can control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.

```{r}
knn8 <- knn2nb(knearneigh(coords, k=8))
knn8

```

Next, nb2listw() is used to convert the nb object into spatial weights object.

```{r}
knn8_lw <- nb2listw(knn8, style = 'B')
summary(knn8_lw)
```

## Computing Gi Statistics

### Gi statistics using fixed distance

```{r}
fips <- order(hunans$County)
gi.fixed <- localG(hunans$GDPPC, wm62_lw)
gi.fixed
```

The output of `localG()` is a vector of G or Gstar values, with attributes "gstari" set to TRUE or FALSE, "call" set to the function call, and class "localG".

The Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.

Next, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.

```{r}
hunan.gi <- cbind(hunans, as.matrix(gi.fixed)) %>% 
  rename(gstat_fixed = as.matrix.gi.fixed.)

```

The code chunk above performs three tasks. 1. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using `as.matrix()`. 2. Next, `cbind()` is used to join `hunans` data frame and `gi.fixed` matrix to produce a new SpatialPolygonDataFrame called `hunan.gi`. Lastly, the field name of the gi values is renamed to `gstat_fixed` by using `rename()`.

### Mapping Gi values with fixed distance weights

The code chunk below maps the Gi values derived using fixed distance weight matrix.

```{r}

gdppc <- qtm(hunans, "GDPPC")

Gimap <- tm_shape(hunan.gi) +
  tm_fill(col = "gstat_fixed", 
          style= "pretty",
          palette = "-RdBu",
          title = "Local Gi") +
  tm_borders(alpha = 0.5)

tmap_arrange(gdppc, Gimap, asp = 1, ncol = 2)
```

### Gi statistics using adaptive distance

We will now compute the Gi values for GDPPC2012 using an adaptive distance weight matrix (i.e. `knb_lw`).

```{r}
fips <- order(hunans$County)
gi.adaptive <- localG(hunans$GDPPC, knn8_lw)
hunan.gi <- cbind(hunans, as.matrix(gi.adaptive)) %>%
  rename(gstat_adaptive = as.matrix.gi.adaptive.)

```

### Mapping Gi values with adaptive distance weights

To visualise the hot and cold spot areas, we will use **tmap** package to map the Gi values derived using adaptive distance weight matrix.

```{r}
gdppc <- qtm(hunans, "GDPPC")

Gimap <- tm_shape(hunan.gi) +
  tm_fill(col = "gstat_adaptive", 
          style="pretty",
          palette="-RdBu",
          title = "Local Gi using adaptive distance weights") +
  tm_borders(alpha = 0.5)

tmap_arrange(gdppc, Gimap, asp=1, ncol=2)

```
